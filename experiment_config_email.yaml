#
# Fair comparison config for PROSE vs Steering - EMAIL TASKS
#
# Key changes from default:
# 1. num_other_sequences: 0 (no cross-source retrieval)
# 2. Constrained vocabulary in inference prompts
# 3. task.name: email_writing
#

parameters:
  base_dir: null  # Base directory to save all models, data.

  # Wandb configs
  wandb_mode: disabled  # Disable wandb for local runs
  wandb_ent: null

  # Logging config
  logging_level: "info"

  # high level configs
  exp_name: ".prose.constrained.fair-comparison-email"
  group_name: "plume.email.fair-comparison"
  seed: 1352

  # Inference parameters
  gpt_temperature: 0.01
  qwen_temperature: 0.7

  # Main experimental parameters
  validation_threshold: 0.5
  num_other_sequences: 0  # CHANGED: No cross-source retrieval (fair comparison with steering)
  num_refinement_steps: 3

  candidate_type: "iterate"
  do_validate_preferences: true
  do_breakdown_preferences_each_iteration: false
  do_breakdown_preferences_after_refinement: true
  include_preferences_in_validate: false
  use_icl: false

  # Related to retrieval
  bertscore_model: "microsoft/deberta-xlarge"
  retrieval_underlying_model: "microsoft/deberta-xlarge"
  retrieval_mode: "exact_match"
  encoder_aggregation: "mean_pool"
  retrieval_threshold: null

  user:
    llm_name: "qwen-2_5-7b"

  task:
    name: "email_writing"  # CHANGED: Email task instead of summarization
    framework: "plume"
    datasets: ["slf5k", "ampere", "paper_tweet"]  # Exclude ccby (elsevier dataset has download issues)
    number_of_examples_per_preference_set: 5  # 5 turns per source
    solve_directly: True

  agent:
    llm_name: "qwen-2_5-7b"
    name: "prose"

  # NEW: Constrained vocabulary for fair comparison
  constrained_vocabulary: true
  max_traits: 3  # Match steering's top-3 selection
